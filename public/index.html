
<html>
  <head>
    <meta charset="utf-8" />

    <title>React_CDN</title>
	<script src="./dist/face-api.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/dist/tf.min.js"></script>
    <script src="https://unpkg.com/react@16/umd/react.development.js"></script>
    <script src="https://unpkg.com/react-dom@16/umd/react-dom.development.js"></script>
    <script src="https://unpkg.com/babel-standalone@6.26.0/babel.js"></script>
  </head>

  <body>
    <div id="root"></div>

    <script type="text/babel">
      // React code will go here
       class App extends React.Component {
           constructor(props) {
                super(props);
                this.state = {
                    label: "Loading..."
                }; 
        		this.modelLoaded = false
        		this.constraints = {
            		audio: false,
            		video: {
                		width: 640,
                		height: 480
            		}
        		};
        		this.emotionModel = {};
        		this.emotion_labels = ["Angry", "Disgust", "Fear", "Happy", "Sad", "Surprise", "Neutral"];
				this.webElement = {};
            }

            componentDidMount(){
				var webElement = document.getElementById('inputVideo');
				this.webElement = webElement
				this.onPlay(webElement);
            	this.run()
            }

		
          	onPlay = async (videoEl) => {
				let scoreThreshold = 0.5;
        		let sizeType = '416';
				let offset_x = 27;
        		let offset_y = 20;

				if (videoEl.paused || videoEl.ended || !this.modelLoaded){
					return false
				}
				
				//console.log("test");
				const {
					width,
					height
				} = faceapi.getMediaDimensions(videoEl)
				const canvas = document.getElementById('overlay');
				canvas.width = width
				canvas.height = height

				const forwardParams = {
					inputSize: parseInt(sizeType),
					scoreThreshold
				}
				const ts = Date.now()
				const result = await faceapi.detectAllFaces(videoEl, new faceapi.TinyFaceDetectorOptions(forwardParams))
				console.result
				if (result.length != 0) {
					const context = canvas.getContext('2d')
					context.drawImage(videoEl, 0, 0, width, height)

					let ctx = context;
					for (var i = 0; i < result.length; i++) {
						ctx.beginPath();
						var item = result[i].box;
						let s_x = Math.floor(item._x+offset_x);
						if (item.y<offset_y){
							var s_y = Math.floor(item._y);
						}
						else{
							var s_y = Math.floor(item._y-offset_y);
						}
						let s_w = Math.floor(item._width-offset_x);
						let s_h = Math.floor(item._height);
						let cT = ctx.getImageData(s_x, s_y, s_w, s_h);
						cT = this.preprocess(cT);
						let pred = this.emotionModel.predict(cT)
						let index = pred.argMax(1).dataSync()[0]
						let label =this.emotion_labels[index];
						//console.log("label:" + label)
						const labelElement = document.getElementById("status");
						labelElement.innerHTML = label
					}
				}else{
					let label = "no Face";
					//console.log("label:" + label)
					const labelElement = document.getElementById("status");
					labelElement.innerHTML = label
				}
				window.requestAnimationFrame(()=>this.onPlay(videoEl));
        	}
      
			// create model
			createModel = async (path) =>{
				let model = await tf.loadLayersModel(path)
				return model
			}
			// load emotion model
			
			loadModel = async (path) =>{
				this.emotionModel = await this.createModel(path)
			}

			preprocess = (imgData) =>{
				return tf.tidy(() => {
					let tensor = tf.browser.fromPixels(imgData).toFloat();
					tensor = tensor.resizeBilinear([100, 100])
					tensor = tf.cast(tensor, 'float32')
					const offset = tf.scalar(255.0);
					// Normalize the image 
					const normalized = tensor.div(offset);
					//We add a dimension to get a batch shape 
					const batched = normalized.expandDims(0)
					return batched
				})
			}

			successCallback(stream) {
				var videoEl = document.getElementById('inputVideo');
				videoEl.srcObject = stream;
			}

			errorCallback(error) {
				console.log("navigator.getUserMedia error: ", error);
			}

			run = async() => {
				this.loadModel('./models/mobilenetv1_models/model.json')
				const Model_url = './models/tiny_face_detector/tiny_face_detector_model-weights_manifest.json'
				await faceapi.loadTinyFaceDetectorModel(Model_url)
				this.modelLoaded = true
				navigator.mediaDevices.getUserMedia(this.constraints)
					.then(this.successCallback)
					.catch(this.errorCallback);
			}
				
       
        
            render() {
                return ( <div className="App">
						 	<h2>
            				<span id="status">{this.state.label}</span>
        					</h2>
            				<video onPlay={()=>this.onPlay(this.webElement)} id="inputVideo" autoPlay ></video>
            				<canvas id="overlay" />
                         </div>
                         );
                 
               }
      }

      ReactDOM.render(<App />, document.getElementById('root'))
    </script>

  </body>
</html>
